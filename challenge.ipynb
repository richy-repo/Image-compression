{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some important notes: \n",
    "\n",
    "Here is the resubmission of this challenge. Thank you for giving me a second attempt. I modify the bitmap usage to run length encoding. In my initial submission, I mistakenly thought bitmap is the same as run length encoding. I did not have a background in compression, and I did the research when doing the quesiton. So I made a mistake in the first submission. In this submission, I have used run length encoding. \n",
    "\n",
    "This challenge is more about data processing, and machine learning techniques. The prepared data could be used to train machine learning or AI models to determine whether a parasite has cancer based on some \"factors/predictors\". I had a strong background in Machine learning and AI, a few years ago. When I had my undergraduate degree, in statiscis and comoputer science, my focus was on AI and machine learning. After the graduation and my master degree, I shifted my focus more to web development, working with cloud, frontend technologies and backend technologies, such as react, angular, node, python, fastapi, express, java spring boot, docker, microservice, etc. So I am no longer proficient in working with data processing and train models. But I did this question as best as I could. \n",
    "\n",
    "Therefore, this kind of challenge is no longer my strength at the moment of this challenge. But I am willing to do some review and refresh my memory if I could be considered as candidates for this type of role. Thank you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1:\n",
    "Image by microscope: One representation is that I can use a bitmap with run length encoding. I initially intended to represent images as 2-d array in python, and I can utilize the GPU's parallel processing in pytorch for further analysis easily. But I notice that the question's requirement is to use as less storage/memory as possible. Therefore, a bitmap with run length encoding would more closely align with the requrements. \n",
    "\n",
    "Since the large amount of consecutive bits are of the same color(white or black), we will use 33 bits to store each run. For example, the 32 bits stores the length of the run, and the last bit stores the color (0 for white, 1 for black).\n",
    "\n",
    "In the worst case, there are 10 billion runs(color alternates). Then there would be 1000000000 * 33 bits. This would be worse than using two d array. But only by a constant factor which is 33. In big O notation, the storage is O(n), suppose n is the number of bits in two d array, which is linear in the worse case for both 2d array representation, and bitmap representation. To translate to bytes, you will simply divde that number by 4\n",
    "\n",
    "But in practice, this representation would be better. Since the situation is likely that in a single row of the 2d array representation, there are only constant number alternations of colors(because the blog is grouped together), then the storage becomes\n",
    "O(log(n)), which would be better than the two d array\n",
    "\n",
    "I would represent dye image in the same way, because given the situation of this challenge, the dye image is also binary(whether the dye is lit or not). Bitmap is great for binary representations. \n",
    "\n",
    "I also reserached if there are intensities of the dye involved, we will use other representations such as tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qestion 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rle_encode_large(binary_array):\n",
    "    \"\"\"\n",
    "    Perform run-length encoding on a large 1D binary numpy array.\n",
    "    This function processes the array in chunks to save memory.\n",
    "    \"\"\"\n",
    "    chunk_size = 10_000_000  # Process 10 million elements at a time\n",
    "    encoded_chunks = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(binary_array), chunk_size), desc=\"Encoding\"):\n",
    "        chunk = binary_array[i:i+chunk_size]\n",
    "        diff = np.diff(chunk)\n",
    "        run_starts = np.where(diff != 0)[0] + 1\n",
    "        run_starts = np.concatenate(([0], run_starts, [len(chunk)]))\n",
    "        run_lengths = np.diff(run_starts)\n",
    "        run_values = chunk[run_starts[:-1]]\n",
    "        encoded_chunks.extend(zip(run_values, run_lengths))\n",
    "    \n",
    "    return encoded_chunks\n",
    "\n",
    "def compress_large_binary_image(binary_image):\n",
    "    \"\"\"\n",
    "    Compress a large 2D binary image using run-length encoding.\n",
    "    \"\"\"\n",
    "    encoded_data = rle_encode_large(binary_image.ravel())\n",
    "    return encoded_data, binary_image.shape\n",
    "\n",
    "def rle_decode_large(encoded_data, shape):\n",
    "    \"\"\"\n",
    "    Decode run-length encoded data back to a large 2D binary numpy array.\n",
    "    This function decodes the data in chunks to save memory.\n",
    "    \"\"\"\n",
    "    total_size = shape[0] * shape[1]\n",
    "    decoded_array = np.zeros(total_size, dtype=np.uint8)\n",
    "    current_index = 0\n",
    "\n",
    "    for value, count in tqdm(encoded_data, desc=\"Decoding\"):\n",
    "        end_index = current_index + count\n",
    "        decoded_array[current_index:end_index] = value\n",
    "        current_index = end_index\n",
    "\n",
    "    return decoded_array.reshape(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_parasite_image(width, height, min_blob_size=0.25, max_blob_size=0.4):\n",
    "    \"\"\"\n",
    "    Generate a simulated microscope image with a single binary blob representing a parasite.\n",
    "    The blob is centered and its size is at least 25% of the smaller image dimension.\n",
    "    \"\"\"\n",
    "    # Create a blank binary image\n",
    "    image = np.zeros((height, width), dtype=np.uint8)\n",
    "    \n",
    "    # Calculate the center of the image\n",
    "    center = (width // 2, height // 2)\n",
    "    \n",
    "    # Calculate the radius\n",
    "    min_dimension = min(width, height)\n",
    "    min_radius = int(min_blob_size * min_dimension // 2)\n",
    "    max_radius = int(max_blob_size * min_dimension // 2)\n",
    "    radius = np.random.randint(min_radius, max_radius)\n",
    "    \n",
    "    # Draw the circle\n",
    "    cv2.circle(image, center, radius, 1, -1)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_veins(parasite_mask, coverage=0.1, num_iterations=5):\n",
    "    \"\"\"Generate vein-like structures within a parasite chunk.\"\"\"\n",
    "    veins = np.zeros_like(parasite_mask, dtype=np.uint8)\n",
    "    \n",
    "    # Calculate the target number of vein pixels\n",
    "    target_vein_pixels = int(np.sum(parasite_mask) * coverage)\n",
    "    \n",
    "    # Start with random points within the parasite\n",
    "    initial_points = np.random.rand(*parasite_mask.shape) < 0.01\n",
    "    veins[initial_points & (parasite_mask == 1)] = 1\n",
    "    \n",
    "    kernel = np.ones((3, 3), np.uint8)\n",
    "    for _ in range(num_iterations):\n",
    "        veins = cv2.dilate(veins, kernel, iterations=1)\n",
    "        veins = veins & parasite_mask\n",
    "        new_points = (np.random.rand(*parasite_mask.shape) < 0.01) & (parasite_mask == 1)\n",
    "        veins = veins | new_points\n",
    "        \n",
    "        if np.sum(veins) >= target_vein_pixels:\n",
    "            break\n",
    "    \n",
    "    return veins\n",
    "\n",
    "def simulate_dye_spread(veins, parasite_mask, num_iterations=10):\n",
    "    \"\"\"Simulate dye spreading from veins within the parasite chunk.\"\"\"\n",
    "    dye = veins.copy()\n",
    "    kernel = np.ones((3, 3), np.uint8)\n",
    "    \n",
    "    for _ in range(num_iterations):\n",
    "        dye = cv2.dilate(dye, kernel, iterations=1)\n",
    "        dye = dye & parasite_mask\n",
    "        dye = dye | veins\n",
    "    \n",
    "    return dye\n",
    "\n",
    "def generate_dye_image(parasite_image, chunk_size=1000, coverage=0.1):\n",
    "    \"\"\"Generate a binary dye image for a single parasite blob using chunks.\"\"\"\n",
    "    height, width = parasite_image.shape\n",
    "    dye_image = np.zeros((height, width), dtype=np.uint8)\n",
    "    \n",
    "    # Calculate total parasite area for coverage calculation\n",
    "    total_parasite_area = np.sum(parasite_image)\n",
    "    total_dye_pixels = int(total_parasite_area * coverage)\n",
    "    current_dye_pixels = 0\n",
    "    \n",
    "    for y in tqdm(range(0, height, chunk_size), desc=\"Generating dye image\"):\n",
    "        for x in range(0, width, chunk_size):\n",
    "            chunk = parasite_image[y:y+chunk_size, x:x+chunk_size]\n",
    "            \n",
    "            if np.sum(chunk) == 0:  # Skip empty chunks\n",
    "                continue\n",
    "            \n",
    "            # Calculate remaining coverage for this chunk\n",
    "            chunk_parasite_area = np.sum(chunk)\n",
    "            chunk_coverage = min(1.0, max(0, (total_dye_pixels - current_dye_pixels) / chunk_parasite_area))\n",
    "            \n",
    "            veins = generate_veins(chunk, coverage=chunk_coverage)\n",
    "            dye_chunk = simulate_dye_spread(veins, chunk)\n",
    "            \n",
    "            dye_image[y:y+chunk_size, x:x+chunk_size] = dye_chunk\n",
    "            current_dye_pixels += np.sum(dye_chunk)\n",
    "            \n",
    "            if current_dye_pixels >= total_dye_pixels:\n",
    "                break\n",
    "        if current_dye_pixels >= total_dye_pixels:\n",
    "            print(\"current_dye_pixels >= total_dye_pixels\")\n",
    "            break\n",
    "    \n",
    "    return dye_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install numpy opencv-python matplotlib\n",
    "# Parameters\n",
    "width, height = 100_000, 100_000\n",
    "# Generate simulated images\n",
    "print(\"Generating parasite image...\")\n",
    "parasite_image = generate_parasite_image(width, height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Compressing parasite image...\")\n",
    "parasite_rle, parasite_shape = compress_large_binary_image(parasite_image)\n",
    "parasites_present = any(value == 1 for value, count in parasite_rle)\n",
    "print(\"Parasites present in RLE:\", parasites_present)\n",
    "parasite_original_size = width * height // 8  # in bytes\n",
    "parasite_compressed_size = sum(len(run) for run in parasite_rle) * 2  # 2 bytes per run (value and count)\n",
    "parasite_compression_ratio = parasite_original_size / parasite_compressed_size\n",
    "print(f\"\\nParasite Image - Original size: {parasite_original_size / 1e9:.2f} GB\")\n",
    "print(f\"Parasite Image - Compressed size: {parasite_compressed_size / 1e9:.2f} GB\")\n",
    "print(f\"Parasite Image - Compression ratio: {parasite_compression_ratio:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generating dye image...\")\n",
    "dye_image = generate_dye_image(parasite_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Compressing dye image...\")\n",
    "dye_rle, dye_shape = compress_large_binary_image(dye_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dye_original_size = width * height // 8  # in bytes\n",
    "dye_compressed_size = sum(len(run) for run in dye_rle) * 2\n",
    "dye_compression_ratio = dye_original_size / dye_compressed_size\n",
    "print(f\"\\nDye Image - Original size: {dye_original_size / 1e9:.2f} GB\")\n",
    "print(f\"Dye Image - Compressed size: {dye_compressed_size / 1e9:.2f} GB\")\n",
    "print(f\"Dye Image - Compression ratio: {dye_compression_ratio:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding: 100%|██████████| 80242/80242 [00:01<00:00, 61064.14it/s] \n",
      "Decoding: 100%|██████████| 14620/14620 [00:01<00:00, 12676.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoding completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:  10%|█         | 10/100 [00:09<01:22,  1.09chunk/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parasite has cancer: True\n",
      "coverage: 0.10033145524309504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def parasite_has_cancer(parasite_rle, dye_rle, chunk_size=10000, threshold=0.1):\n",
    "    \"\"\"\n",
    "    Determine if a parasite has cancer based on dye coverage.\n",
    "    \n",
    "    Args:\n",
    "    parasite_image (np.ndarray): Binary image of the parasite (1 for parasite, 0 for background)\n",
    "    dye_image (np.ndarray): Binary image of the dye (1 for dye presence, 0 for absence)\n",
    "    chunk_size (int): The size of the chunks to process the images\n",
    "    threshold (float): The dye coverage threshold for cancer diagnosis (default: 0.1)\n",
    "    \n",
    "    Returns:\n",
    "    bool: True if the parasite has cancer (dye coverage > threshold), False otherwise\n",
    "    float: The actual dye coverage\n",
    "    \"\"\"\n",
    "    \n",
    "    parasite_image= rle_decode_large(parasite_rle, parasite_shape)\n",
    "    dye_image = rle_decode_large(dye_rle, dye_shape)\n",
    "    print(\"Decoding completed\")\n",
    "    # Ensure the images are binary and have the same shape\n",
    "    assert parasite_image.shape == dye_image.shape, \"Parasite and dye images must have the same shape\"\n",
    "    # assert set(np.unique(parasite_image)).issubset({0, 1}), \"Parasite image must be binary\"\n",
    "    # assert set(np.unique(dye_image)).issubset({0, 1}), \"Dye image must be binary\"\n",
    "    \n",
    "    # Initialize the total areas\n",
    "    total_parasite_area = 0\n",
    "    total_dye_area_in_parasite = 0\n",
    "    \n",
    "    # Get the dimensions of the images\n",
    "    height, width = parasite_image.shape\n",
    "    \n",
    "    # Calculate the total number of chunks to process for progress tracking\n",
    "    total_chunks = (height // chunk_size + (height % chunk_size > 0)) * (width // chunk_size + (width % chunk_size > 0))\n",
    "    \n",
    "    # Process the images in chunks with progress information\n",
    "    for y in tqdm(range(0, height, chunk_size), desc=\"Processing rows\", total=total_chunks, unit=\"chunk\"):\n",
    "        for x in range(0, width, chunk_size):\n",
    "            # Define the chunk boundaries\n",
    "            y_end = min(y + chunk_size, height)\n",
    "            x_end = min(x + chunk_size, width)\n",
    "            \n",
    "            # Extract the chunks\n",
    "            parasite_chunk = parasite_image[y:y_end, x:x_end]\n",
    "            dye_chunk = dye_image[y:y_end, x:x_end]\n",
    "            \n",
    "            # Calculate the areas in the chunk\n",
    "            parasite_area = np.sum(parasite_chunk)\n",
    "            dye_area_in_parasite = np.sum(dye_chunk & parasite_chunk)\n",
    "            \n",
    "            # Update the total areas\n",
    "            total_parasite_area += parasite_area\n",
    "            total_dye_area_in_parasite += dye_area_in_parasite\n",
    "    \n",
    "    # Ensure the parasite exists in the image\n",
    "    if total_parasite_area == 0:\n",
    "        raise ValueError(\"No parasite detected in the image\")\n",
    "    \n",
    "    # Calculate the dye coverage\n",
    "    dye_coverage = total_dye_area_in_parasite / total_parasite_area\n",
    "    \n",
    "    # Determine if the parasite has cancer\n",
    "    has_cancer = dye_coverage > threshold\n",
    "    \n",
    "    return has_cancer, dye_coverage\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "has_cancer, dye_coverage = parasite_has_cancer(parasite_rle, dye_rle)\n",
    "\n",
    "print(f\"Parasite has cancer: {has_cancer}\")\n",
    "print(f\"coverage: {dye_coverage}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quesiton 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am using a laptop, and I got kernel error when I tried to generate images of 100000*100000. I believe the memory runs out. So for question 3, I had to use the improved method of using trunks for processing image of 100k. So my answer for question 3 is the improved one. \n",
    "\n",
    "I am running this on my local computer. Normally, I would run it in cloud env such as google colab, which can utilize GPU for extrememly fast processing. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I could not come up with effective compression for this question, so I used both claud ai and chatgpt for an answer\n",
    "\n",
    "Based on my inqueries, Quadtree compression could be an answer. \n",
    "\n",
    "My understanding is that we represent the image in tree structure. Each node represent a group of bits with the value. We divide the square image into four quads, and keep diving until all bits in a quads are of the same value. Then such quad becomes a leaf. This could be an effective representation of the images besides the bitmap. \n",
    "\n",
    "But given the setting of this challenge, I belive quadtree does not necessarily produce a better run time/storage saving. \n",
    "\n",
    "I had experience using google colab before. When I trained machine learning model before and turned large neural network, I typically run the cloud using google colab and run on the GPU. When budget is sufficent, upgrading the hardware might be the most effective approach than dealing with compression techniques. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6:\n",
    "\n",
    "Tools I used for this challenge: \n",
    "\n",
    "Google(searching)\n",
    "Chatgpt(explaining the data structure, such as bitmap)\n",
    "\n",
    "I am good at writing prompts to the LLM, and ask for answers to my questions in my daily development task, and this can greatly increase my efficiency.\n",
    "\n",
    "I used chatgpt for the API of the libraries, such as ny functions, and some nuances.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
